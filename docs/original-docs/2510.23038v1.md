# Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning

**Authors:** Ran XuÂ¹'Â²*, Jingjing ChenÂ², Jiayu YeÂ², Yu WuÂ², Jun YanÂ³, Carl YangÂ¹ and Hongkun YuÂ²

Â¹Emory University, Â²Google, Â³Google Cloud AI Research  
*Work done during an internship at Google.

**Published:** 2025-10-28  
**arXiv:** 2510.23038v1 [cs.CL] 27 Oct 2025

## Abstract

Large Language Models (LLMs) are widely used as judges to evaluate response quality, providing a scalable alternative to human evaluation. However, most LLM judges operate solely on intrinsic text-based reasoning, limiting their ability to verify complex constraints or perform accurate computation. Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks, we propose TIR-Judge, an end-to-end RL framework for training LLM judges that integrates a code executor for precise evaluation.

TIR-Judge is built on three principles: 
1. diverse training across verifiable and non-verifiable domains
2. flexible judgment formats (pointwise, pairwise, listwise) 
3. iterative RL that bootstraps directly from the initial model without distillation

On seven public benchmarks, TIR-Judge surpasses strong reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and achieves listwise performance comparable to Claude-Opus-4 despite having only 8B parameters. Remarkably, TIR-Judge-Zero â€“ trained entirely without distilled judge trajectories, matches the performance of distilled variants, demonstrating that tool-augmented judges can self-evolve through iterative reinforcement learning.

**Keywords:** LLM-as-a-judge, Reinforcement Learning, Tool-Integrated Reasoning

## 1. Introduction

Large Language Model (LLM)-based judges are emerging as a critical component in the LLM ecosystem, typically used with scoring and ranking model outputs. This evaluation capability is essential at multiple stages of LLM development: during post-training, judges provide preference signals for alignment; at inference time, judges verify and select responses through best-of-N decoding; and during evaluation, judges deliver reliable assessments without manual human assessment. Thus, training accurate LLM-based judges is of great importance for building powerful language models.

![Example of LLM judge augmented with code execution](image_placeholder_figure_1)

*Figure 1: An example of LLM judge augmented with code execution, enabling precise judgments.*

Classical evaluation with reward models often outputs scores directly, which cannot fully harvest the inherent reasoning capability of LLM. Recent progress in generative reward modeling and reinforcement learning equips judges with thinking before producing final predictions. While these approaches enhance judge quality by equipping LLMs with long chains of textual reasoning traces, they remain inherently limited in scenarios that require precise computation or symbolic reasoning â€“ capabilities that are much more challenging for text-only models.

Recent advances in LLM tool-use provide a promising avenue to overcome the limitations of text-only judges. By granting access to executable interfaces for enumeration, verification, and computation, tools enable exact validation of reasoning steps rather than relying on potentially error-prone text-based inference. For example, code execution can automatically verify outputs on certain instructions or check intermediate calculations in math reasoning. Early attempts have also explored equipping LLM judges with tool-use abilities, but these approaches reveal two major limitations: 

1. **Inference-time restriction**: most methods integrate tool-use only at the inference stage, preventing deeper integration between reasoning processes and tool execution
2. **Narrow task coverage**: many are tailored to specific domains or specialized task types, which limits their applicability in general-purpose judging scenarios

These gaps highlight the need for robust judges that tightly couple reasoning with tool execution and be optimized end-to-end.

### Contributions

Our contribution can be summarized as follows:

- We introduce **TIR-Judge**, a tool-integrated framework for training LLM-based judges with end-to-end multi-turn reinforcement learning. To the best of our knowledge, this is the first approach that jointly optimizes reasoning and tool-use for training LLM-based judges via RL.

- We design several key strategies to fully exploit the power of reinforcement learning, including task diversification across verifiable and non-verifiable domains, flexible judgment formats (pointwise, pairwise, listwise), as well as an iterative RL scheme that unlocks self-improvement in tool use even without distillation.

- We evaluate TIR-Judge on seven public benchmarks covering diverse tasks and input formats. TIR-Judge consistently outperforms strong reasoning-based judges, achieving gains of up to 6.4% (pointwise) and 7.7% (pairwise). Moreover, TIR-Judge shows strong parameter efficiency: With only 8B parameters, it surpasses the 32B reasoning reward models on the PPE dataset, and reaches 96% of the performance of Claude-Opus-4 in the listwise setting in RewardBench 2. Interestingly, TIR-Judge-Zero, the judge trained without any distillation, achieves a 1.2% gain over its distilled counterpart at 4B scale, highlighting the power of RL to bootstrap reasoning and tool-use capabilities.

## 2. Related Works

### Reasoning-Enhanced Reward and Judge Models

A growing line of work strengthens reward models (RMs) and judges by explicitly training them to reason before issuing a score or decision. Generative Verifiers frame verification as next-token prediction with chain-of-thought, achieving notable gains on mathematical and algorithmic reasoning tasks. Other approaches strengthen judgment quality by generating critiques prior to reward prediction, leveraging multi-round preference optimization to progressively refine judge capability, or teaching models to first propose evaluation plans or rubrics before producing a final judgment.

More recently, reinforcement learning has been used to optimize LLM-as-a-Judge, encouraging longer, higher-quality reasoning and reducing bias across pairwise and pointwise settings. While effective at strengthening textual reasoning and planning, these methods remain limited to reasoning expressed in natural language and often focus primarily on pairwise judgment.

### Tool-Assisted Reward and Judge Models

Another line of work augments judges with external tools. Li et al. incorporate verifiable signals alongside preference data for judge training, though primarily within tool-use scenarios. Zhuge et al. evaluate agentic judge capabilities in agent settings, and Agentic Reward Modeling integrates human preferences with correctness checks to construct more reliable rewards. Findeis et al. study whether external tools (e.g., code execution, web search) improve LLM-as-a-Judge annotations, reporting consistent but task-dependent gains. However, these approaches largely depend on prompted tool use rather than training judges to learn when and how to invoke tools and to integrate their outputs into decisions.

### Reinforcement Learning for Tool-integrated Reasoning

Recent work explores reinforcement learning for TIR. Several studies train LLMs to interleave reasoning with code execution, discovering strategic tool-use policies that improve math and programming tasks. Several works extend this paradigm by interleaving reasoning steps on search agents, web agents and coding agents. Other studies focus on optimal reward design for TIR or provide theoretical analysis of its advantages.

## 3. Preliminaries

### Problem Setup

We consider the task of LLM-based judgment: given a user prompt x âˆˆ X and n model-generated responses Y = {yâ‚, yâ‚‚, ..., yâ‚™}, the goal is to evaluate the quality of responses for the prompt. The judge model JÎ¸ produces an evaluation output conditioned on (x, Y). In this work, we consider three evaluation settings:

1. **Pointwise evaluation**: given (x, y), the judge assigns a scalar score, JÎ¸(x, y) = sÎ¸(x, y) âˆˆ â„
2. **Pairwise evaluation**: given (x, yâ‚, yáµ¦), the judge selects the preferred response, JÎ¸(x, yâ‚, yáµ¦) = arg max_{iâˆˆ{a,b}} sÎ¸(x, yáµ¢), where sÎ¸ denotes a learned scoring function. This is also the most common evaluation setting
3. **Listwise evaluation**: given (x, Y) with n > 2, the judge returns the index of the best response, JÎ¸(x, Y) = arg max_{iâˆˆ{1,...,n}} sÎ¸(x, yáµ¢)

These settings unify a broad range of evaluations under a common frameworkÂ¹.

### Tool-Augmented Judge

We extend the judge with the ability to call an external Python execution environment I. For the prompt x âˆˆ X, At step k, the judgment trajectory sâ‚– is represented as sâ‚– = {râ‚, câ‚, oâ‚, ..., râ‚–, câ‚–, oâ‚–}, where ráµ¢ is a natural language reasoning step, cáµ¢ is a generated code, and oáµ¢ = I(cáµ¢) is the execution result of cáµ¢. The iterative process is defined as:

(râ‚–, câ‚–) âˆ¼ J(x âŠ• sâ‚–â‚‹â‚), oâ‚– = I(câ‚–), sâ‚– = sâ‚–â‚‹â‚ âŠ• râ‚– âŠ• câ‚– âŠ• oâ‚–    (1)

This cycle continues until the judge produces a final prediction aáµ¢ âˆ¼ J(x âŠ• sâ‚œ) with T being the final step. Unlike traditional text-only reasoning, the trajectory now interleaves reasoning, code execution, and tool feedback, enabling the judge to ground its decision in verifiable evidence.

## 4. Training TIR-Judge

![Overall framework of TIR-Judge variants](image_placeholder_figure_2)

*Figure 2: Overall framework of TIR-Judge variants. TIR-Judge natively supports tool use during judgment and is designed to handle diverse input formats.*

We now describe the training procedure for TIR-Judge, which consists of four components: (1) data collection and filtering for RL, (2) the RL framework for training judges with integrated code execution tools, (3) reward design for RL, and (4) cold-start and iterative training strategies in RL.

### 4.1. Data Collection and Filtering

High-quality training data is crucial for RL with tool-augmented judges. Since judgment requires both prompts and candidate responses, we curate a collection of (prompt, responses) tuples spanning multiple tasks. Our corpus integrates both human-annotated preference data and automatically generated synthetic pairs to ensure diversity and scalability.

#### Real Preference Pairs

We sample human-labeled preference pairs from a variety of domains:
- **general helpfulness** â€” HelpSteer 3
- **reasoning** â€” UltraInteract, S1
- **coding** â€” CodeRM
- **instruction following (IF)** â€” preference pairs from Tulu 3
- **safety** â€” Safe-RLHF

Each prompt is paired with one preferred (chosen) response and one or more rejected responses.

#### Synthetic Preference Pairs

Because reasoning preference data is often limited in scale, we augment the corpus with synthetic preference pairs generated from verifiable prompts. For each prompt, we sample responses from multiple open-source models, including Qwen3-8B/14B, Gemma-2-9B, and Gemma-3-12B. The responses are automatically evaluated against verifiable functions (for IF tasks) or ground-truth solutions (for reasoning tasks) to form preference pairs.

For reasoning, we employ MATH and DAPO-Math for math domain and WebInstruct, and Loong for general domain, both of which provide ground-truth solutions for exact verification.

In total, our dataset comprises approximately 26k preference pairs, including pointwise, pairwise, and listwise annotations, covering diverse domains such as helpfulness, reasoning, coding, safety, and verifiable instruction following. We apply strict 8-gram decontamination to eliminate any overlap between training prompts and evaluation benchmarks. This diverse mixture of data provides a strong foundation for training robust tool-augmented judges.

### 4.2. Tool-Integrated RL with Verifiable Rewards

#### Overall Framework

We adopt DAPO, an improved variant of GRPO, for training the LLM judge J parameterized by Ï€Î¸. Given a promptâ€“answer pair (q, a), we first sample a group of G rollouts {sáµ¢}á´³áµ¢â‚Œâ‚ from the current policy Ï€Î¸old. Each rollout sáµ¢ is assigned a scalar reward Ráµ¢ = R(sáµ¢, a) with access to the oracle answer a. The policy Ï€Î¸ is then updated with the following clipped policy gradient objective:

J(Î¸) = ğ”¼_{(q,a)âˆ¼ğ’Ÿ,{sáµ¢}á´³áµ¢â‚Œâ‚âˆ¼Ï€Î¸old(Â·|q)} [1/âˆ‘á´³áµ¢â‚Œâ‚|sáµ¢| âˆ‘á´³áµ¢â‚Œâ‚âˆ‘|sáµ¢|â‚œâ‚Œâ‚ min(ráµ¢,â‚œ(Î¸)Ã‚áµ¢,â‚œ, clip(ráµ¢,â‚œ(Î¸), 1 âˆ’ Îµâ‚—â‚’w, 1 + Îµâ‚•áµ¢gâ‚•)Ã‚áµ¢,â‚œ) âˆ’ Î²ğ’Ÿâ‚–â‚—(Ï€Î¸âˆ¥Ï€áµ£â‚‘f)]

s.t. 0 < |{sáµ¢ : is_equivalent(a, sáµ¢)}| < G

where ráµ¢,â‚œ(Î¸) = Ï€Î¸(sáµ¢,â‚œ|q,sáµ¢,<â‚œ)/Ï€Î¸old(sáµ¢,â‚œ|q,sáµ¢,<â‚œ) is the token-level weight, Ã‚áµ¢,â‚œ = (Ráµ¢âˆ’mean({Ráµ¢}á´³áµ¢â‚Œâ‚))/std({Ráµ¢}á´³áµ¢â‚Œâ‚) is the advantage at the token level, and is_equivalent step filters out the prompts with accuracy equal to 1 and 0. The hyperparameters Îµâ‚—â‚’w and Îµâ‚•áµ¢gâ‚• control the clipping range for importance weights, while Î² regulates the KL divergence penalty to stabilize training.

#### Additional Designs

Beyond standard RL training, we implement two enhancements to stabilize tool-augmented judgment:

1. **Error Message Processing**: We truncate the outputs from Interpreter I to only the final error line to avoid excessive context length while preserving useful feedback in sâ‚–
2. **Sandbox Output Masking**: Since execution results oáµ¢ = I(cáµ¢) may cause the model to overfit by memorizing outputs, we mask oáµ¢ during loss computation. This prevents reliance on exact strings and improves training stability.

#### Reward Designs

To effectively facilitate multi-turn RL with code execution, we design a structured reward covering three aspects:

**(i) Correctness Reward Râ‚‘**: This component measures whether the judge's prediction aligns with the reference preference label. Let x denote the prompt, Y = {yâ‚, ..., yâ‚™} the candidate responses, and l the ground-truth preferred response. The reward is defined as:

Râ‚‘ = {
  ğ•€[sÎ¸(x, yâ‚šâ‚’â‚›) > sÎ¸(x, yâ‚™â‚‘g)], for pointwise evaluation,
  ğ•€[JÎ¸(x, Y) = l], for pairwise or listwise evaluation,
  0, otherwise,
}

where ğ•€(Â·) is the indicator function, sÎ¸(x, y) denotes the judge's scoring function, and JÎ¸(x, Y) is the predicted best response under the judge's policy. Intuitively, Râ‚‘ = 1 if the judge's decision matches the ground-truth preference, and Râ‚‘ = 0 otherwise.

**(ii) Format Reward Râ‚“**: To ensure reliability, the judge is required to strictly follow a predefined structured output format. Specifically, prediction scores must be enclosed within `<score>` and `</score>` tags, the preference label must be wrapped in `<preference>` and `</preference>` tags, and all code segments must be enclosed using ```python and ```. In addition, to accommodate both reasoning and non-reasoning tasks and discourage unnecessary tool calls, we introduce a heuristic: for safety and general helpfulness prompts, a positive format reward is granted only if the model produces a valid output without invoking tools. Formally, Râ‚“ = 1 if the output satisfies all formatting constraints (and the no-tool heuristic when applicable), and Râ‚“ = 0 otherwise.

**(iii) Tool-Specific Reward Râ‚œ**: We encourage accurate and efficient tool use by penalizing errors or excessive executions. We set the max number of tool calls per trajectory to 3, and set Râ‚œ = 1 only when code blocks cáµ¢ are error-free and within the call budget; otherwise Râ‚œ = 0.

The final reward R is defined as a combination of correctness, format, and tool-specific rewards and assigns full credit only when correctness, format, and tool-use are all satisfied:

R = Râ‚‘ Ã— [0.1 + 0.9 ğ•€[Râ‚œ = 1 âˆ§ Râ‚“ = 1]]    (3)

### 4.3. Training Strategies for RL

Directly applying RL often leads to suboptimal outcomes, as the base model lacks sufficient reasoning and tool-use capability. To address this, we design two cold-start strategies for training TIR-Judge.

#### Distillation from Teacher Models (TIR-Judge-Distill)

We leverage a stronger teacher, Gemini-2.5-Flash with code execution, to generate high-quality trajectories via rejection sampling. For each user prompt x and corresponding Y, we collect a trajectory s and a final prediction a as (x, Y, s, a) âˆ¼ J. Only trajectories that produce correct answers are retained, yielding a dataset Tâ‚›â‚“â‚œ = {(x, Y, s, a) | R(s, a, l) = 1}. Then the student judge is trained via supervised fine-tuning (SFT) with objective:

â„’â‚›â‚“â‚œ = âˆ’ğ”¼â‚â‚“,Ï„â‚âˆ¼Tâ‚›â‚“â‚œ [âˆ‘|y|áµ¢â‚Œâ‚ log fÎ¸(Ï„áµ¢| Ï„<áµ¢, x)]

where Ï„ = (s, a) is the target trajectory with reasoning and code steps. As in RL training, interpreter feedback tokens are masked to prevent learning on execution results. In total, we collect about 10k tool-integrated trajectories for SFT, which serve as the initialization before reinforcement learning.

#### Iterative Training without Distillation (TIR-Judge-Zero)

Beyond teacher distillation, we investigate whether tool-augmented judges can improve purely through self-bootstrapping. The process alternates between RL, rejection sampling, and supervised fine-tuning.

Starting from the initial model Ï€Î¸â‚€, we first obtain the checkpoint Ï€Î¸â‚ via direct RL on training data as Ï€Î¸â‚ â† RL(Ï€Î¸â‚€). Then, for each prompt x, we sample multiple trajectories from Ï€Î¸â‚ as {sáµ¢}á´³áµ¢â‚Œâ‚ âˆ¼ Ï€Î¸â‚œ(Â· | x) (G = 4 in our study), where each trajectory contains reasoning, code, and execution results: sáµ¢ = {râ‚, câ‚, oâ‚, ..., râ‚–, câ‚–, oâ‚–}. We retain only valid trajectories that (i) produce the correct answer l, (ii) satisfy the output format, and (iii) execute without interpreter errors as Tâ‚œ = {(x, s, a) | R(s, a, l) = 1}. To promote efficiency, for each prompt we further keep only one trajectory, preferring the shortest response or the one with the fewest tool calls. The dataset Tâ‚œ is then used for SFT, and the fine-tuned model initializes the next RL round. After each cycle, we select the best checkpoint based on held-out validation accuracy and repeat the RS â†’ SFT â†’ RL loop:

Tâ‚œâ‚Šâ‚ â† RS(Ï€Î¸â‚œ), Ï€Î¸â‚œâ‚Šâ‚ â† SFT(Ï€Î¸â‚œ, Tâ‚œâ‚Šâ‚), Ï€Î¸â‚œâ‚Šâ‚ â† RL(Ï€Î¸â‚œâ‚Šâ‚).

This iterative process allows TIR-Judge-Zero to progressively bootstrap stronger reasoning and tool-use capabilities entirely from a initial model and facilitates self-improvement without distillation.

## 5. Experiments

### 5.1. Experiment Setups

#### Evaluation Datasets

Following prior work, we focus on reasoning tasks, evaluating TIR-Judge on PPE Correctness. We additionally consider two more challenging datasets on judges: IFBench for instruction-following and CodeJudgeBench for code generation. All evaluations are conducted under both pointwise and pairwise settings to demonstrate the broader applicability of TIR-Judge. We also evaluate on general-domain judge benchmarks, where reasoning constitutes a subset, including RewardBench, RM-Bench and JudgeBench for pointwise/pairwise evaluation, and RewardBench 2 for listwise evaluation.

#### Implementation Details

We use Qwen3-8B and Qwen3-4B-Instruct-2507 as backbones, without enabling thinking mode, and implement training with Verl-Tool. For SFT, we train with batch size 64, learning rate 2e-6, context length 8192, for 1 epoch. For RL, we set the micro batchsize per gpu to 4, mini batchsize to 128 and number of rollout to 8. We set Îµâ‚—â‚’w = 0.2, Îµâ‚•áµ¢gâ‚• = 0.3, Î² = 0.01, max response length to 8192, learning rate 1e-6 and train for 2 epochs. 5% of the prompts from each task were hold-out for validation. The experiments are run with 8 NVIDIA H100 80G GPUs. For data collection, we generate 2 rollouts for each model with t = 0.9, p = 0.95. No external feedback (e.g., GPT annotations) is used.

#### Baselines

We consider the following baselines:

1. **Off-the-shelf LLM judges**: GPT-4o, GPT-o1-mini, Deepseek-R1, Claude 3.5, Gemini-2.5-Flash, Qwen-3
2. **Standard Reward Models**: Armo-RM, Skywork-Reward-Gemma-2, Deepseek-BTRM
3. **Text-based Judges trained with RL**: Deepseek-GRM, J1, RM-R1, RRM and Think-RM
4. **Tool-augmented Judges**: Gemini-2.5-Flash-Tool, AgentRM, and Qwen-3 with the same tool as TIR-Judge.

### 5.2. Main Experiment Results

#### Experiments for Pointwise/Pairwise Judging tasks

Table 1 shows the main results of TIR-Judge on six judge benchmarks. From the results, we have the following key observations:

1. **TIR-Judge achieves strong judging accuracy** compared to baselines. Notably, on the PPE benchmark, TIR-Judge outperforms baselines with similar sizes by 4.8%-9.9% for pointwise judging and 4.5%-8.8% for pairwise judging. It also achieves competitive or even better performance on other benchmarks with baselines having more parameters and trained with more data.

2. **RL is critical for boosting tool-use capability** for judges: Simply augmenting Qwen-3 models with code execution yields negligible (<1%) or even negative gains. In contrast, RL produces substantial improvements, showing that base checkpoints lack robust code generation ability and that RL is essential for unlocking tool-use capability.

3. **Iterative RL is surprisingly effective** to reduce the need for distillation: Comparing TIR-Judge-Zero with TIR-Judge-Distill, we find that TIR-Judge-Zero delivers comparable or better performance, outperforming the distilled variant on 4/6 benchmarks (pointwise) and 3/6 benchmarks (pairwise).

#### Experiments on Listwise Judging tasks

| Datasets | IF | Math | Fact | Focus | Safety | Avg. |
|----------|-----|------|------|-------|---------|------|
| Claude-Opus-4 | 41.9 | 74.9 | 82.7 | 86.2 | 89.5 | 76.5 |
| Gemini-2.5-flash-Preview | 55.3 | 81.1 | 65.7 | 86.7 | 90.9 | 75.9 |
| **TIR-Judge-Zero 8B** | 45.6 | 84.1 | 64.8 | 89.5 | 82.7 | **73.4** |
| TIR-Judge-Distill 8B | 58.1 | 72.7 | 63.8 | 81.4 | 82.0 | 71.6 |

*Table 2: Results on 5 tasks in RewardBench2, sorted by average performance.*

We further evaluate TIR-Judge on RewardBench2 under listwise judge setting, where the input contains one chosen and multiple rejected responses. TIR-Judge achieves strong performance, matching 96% performance of Claude-Opus-4, the current best model on the leaderboard, despite being 8B parameter only. The advantage is more notable on tasks such as instruction following and mathematical reasoning, where TIR-Judge's integration of code execution provides a clear gain.

### 5.3. Additional Studies

#### Diverse Data Mixture is essential for RL

![The effect of different data mixture used in RL training](image_placeholder_figure_3)

*Figure 3: The effect of different data mixture used in RL training of TIR-Judge-Zero.*

We study the impact of task composition in RL. Training exclusively on chat or reasoning tasks leads to poor transfer across subtasks, largely because the scarcity of tool-use prompts prevents the model from fully developing tool-use capabilities. In contrast, unifying tasks â€“ both with and without tool use â€“ into a single training pipeline leads to improved generalization.

#### Tool Use vs. Text-Only

![Experimental results comparing tool-augmented judges against text-only judges](image_placeholder_figure_4)

*Figure 4: Experimental results comparing tool-augmented judges against text-only judges under the same training data and settings, as well as the best-of-N inference performance.*

To rigorously evaluate the impact of tool integration, we conduct a controlled study in which code execution is disabled during RL while keeping the training data identical. Tool-augmented models achieve consistently higher accuracy on reasoning and IF benchmarks, while text-only models perform slightly better on text-centric tasks such as Chat and Safety in RMBench. These comparisons highlight the strength of tool-augmented judges for reasoning, and further suggest that mixing prompts from both tool-use and nonâ€“tool-use settings maintains robust performance without sacrificing much on cases where tools are unnecessary.

#### Efficiency Studies

![Study on Inference Efficiency](image_placeholder_figure_6)

*Figure 6: Study on Inference Efficiency.*

We evaluate the efficiency of TIR-Judge against several baselines. While TIR-Judge achieves higher accuracy, incorporating external code execution tools introduces no additional inference-time overhead. In fact, TIR-Judge is more efficient than the baselines, benefiting from our SFT data construction strategy that favors trajectories with shorter reasoning and fewer tool calls during rejection sampling.

#### Iterative RL progressively improves TIR-Judge-Zero

![Accuracy of TIR-Judge across different training stages](image_placeholder_figure_5)

*Figure 5: Accuracy of TIR-Judge across different training stages.*

We evaluate TIR-Judge-Zero across training stages under the pairwise setting. We observe substantial gains after the first round of RL. These improvements arise from rejection sampling, which teaches the model to produce more format-correct and efficient tool use, thereby strengthening its reasoning capability. Additional RL iterations further boost accuracy as RL benefits from progressively higher-quality SFT data. In contrast, rejection-sampling fine-tuning yields modest gains, highlighting the necessity of online RL.

### 5.4. Best-of-N Evaluation on Policy Models

We conduct parallel test-time compute scaling experiment to study whether TIR-Judge can improve the downstream performance of the policy model, where we conduct a study on reward-guided best-of-N inference over datasets from multiple domains including AIME-2024, AIME-2025, BigCodeBench and IFEval.

TIR-Judge consistently surpasses both Majority Voting (Self-Consistency) and RRM by clear margins, demonstrating its effectiveness. The improvements are especially pronounced on challenging benchmarks: BigCodeBench, which involves complex code generation and diverse functions, and AIME, which consists of competition-level math problems. On these tasks, TIR-Judge achieves absolute gains of 3.9â€“6.7% over RRM. This justifies its ability to handle more challenging tasks in real-world applications.

### 5.5. Case Studies

Table 3 presents an example from the IFEval subset of the PPE benchmark. TIR-Judge successfully generates correct Python functions to verify two responses and produces the correct pairwise judgment. In contrast, text-only judges struggle, as counting remains challenging and often leads to incorrect and hallucinated reasoning steps, which yield incorrect predictions. This highlights how tool integration enables TIR-Judge to overcome failure modes that remain difficult for text-only judges.

## 6. Conclusion

In this work, we introduce TIR-Judge, the first tool-integrated framework for training LLM judges with end-to-end reinforcement learning. Different from prior works on text-only judges, TIR-Judge tightly couples reasoning with code execution to enable judges to perform precise verification and computation. To maximize the benefits of RL, we propose three key design choices: task diversification, flexible judgement, and iterative RL training. Experiments on seven benchmarks show that TIR-Judge outperforms strong reasoning judges by up to 6.4% (pointwise) and 7.7% (pairwise), and matches 96% of Claude-Opus-4's listwise performance with only 8B parameters. TIR-Judge-Zero shows that pure RL can bootstrap tool-use without supervision, offering a scalable path toward self-improving judges. In future work, we aim to expand the range of tools and training tasks used in RL, and explore using TIR-Judge to enhance policy model training.

## Acknowledgement

We would like to thank Jing Nathan Yan, Zhengzhe Yang, Yuchen Zhuang from Google DeepMind for useful discussions.

## References

[The references section contains numerous academic citations - full bibliographic details available in original document]

---

*Â¹Note that in our work, the reference answer is unseen during evaluation, different from the verification setting where the reference answer is also a part of the input.*

---

**Note**: This markdown conversion preserves the structure and content of the academic paper while making it readable in markdown format. Images are referenced as placeholders since the actual figures cannot be embedded in this text format.