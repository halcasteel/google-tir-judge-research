# TIR-Judge Research Project

A research project investigating **Tool-Integrated Reinforcement Learning for LLM Judges** based on Google Research's breakthrough paper.

## ğŸ¯ Project Overview

This project explores the TIR-Judge framework, which combines Large Language Model judges with code execution capabilities through reinforcement learning. The approach enables more accurate evaluation of responses by allowing judges to perform precise computation and verification.

## ğŸ“š Research Focus

### Core Innovation: TIR-Judge
- **Tool Integration**: LLM judges augmented with Python code execution
- **Reinforcement Learning**: End-to-end training using DAPO (improved GRPO)
- **Self-Bootstrapping**: TIR-Judge-Zero learns without teacher distillation
- **Multi-Format Evaluation**: Supports pointwise, pairwise, and listwise judgment

### Key Findings from Original Research
- **6.4% improvement** in pointwise evaluation vs reasoning-based judges
- **7.7% improvement** in pairwise evaluation
- **96% performance** of Claude-Opus-4 with only 8B parameters
- **Self-improvement** without distillation through iterative RL

## ğŸ“ Project Structure

```
./
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ original-docs/          # Source papers and materials
â”‚   â”‚   â”œâ”€â”€ 2510.23038v1.pdf   # Original research paper
â”‚   â”‚   â”œâ”€â”€ 2510.23038v1.md    # Converted markdown
â”‚   â”‚   â””â”€â”€ *.md               # Research notes
â”‚   â”œâ”€â”€ analysis/               # Research analysis
â”‚   â””â”€â”€ summaries/              # Key findings
â”œâ”€â”€ src/                        # Implementation code
â”œâ”€â”€ data/                       # Datasets and experiments
â”œâ”€â”€ results/                    # Experimental outputs
â”œâ”€â”€ notebooks/                  # Jupyter analysis notebooks
â”œâ”€â”€ scripts/                    # Automation scripts
â”œâ”€â”€ CLAUDE.md                   # Project guide for Claude
â””â”€â”€ README.md                   # This file
```

## ğŸ”¬ Research Areas

### 1. Tool-Integrated Reasoning (TIR)
- Code execution sandbox integration
- Multi-turn reasoning with tool feedback
- Verifiable constraint checking

### 2. Reinforcement Learning Framework
- Reward design (correctness + format + tool-use)
- Iterative training strategies
- Self-bootstrapping vs distillation

### 3. Evaluation Methodologies
- **Pointwise**: Individual response scoring
- **Pairwise**: Response comparison
- **Listwise**: Multi-response ranking

## ğŸš€ Getting Started

### Prerequisites
```bash
# Python environment
python >= 3.8
pytorch >= 1.9
transformers >= 4.20
```

### Quick Setup
```bash
# Clone and setup
git clone <repository-url>
cd tir-judge-research

# Install dependencies
pip install -r requirements.txt

# Explore the research
jupyter notebook notebooks/
```

## ğŸ“Š Key Results to Replicate

| Model | Params | PPE Pointwise | PPE Pairwise | RewardBench2 |
|-------|--------|---------------|--------------|--------------|
| TIR-Judge-Zero 8B | 8B | 67.8% | 76.6% | 73.4% |
| TIR-Judge-Distill 8B | 8B | 70.9% | 72.2% | 71.6% |
| Claude-Opus-4 | ~175B | - | - | 76.5% |

## ğŸ“ Research Questions

1. **Tool vs Text**: How does tool integration compare to pure text reasoning?
2. **Self-Learning**: Can judges bootstrap without teacher distillation?
3. **Scaling**: How does performance scale with model size and data?
4. **Generalization**: Do tool-augmented judges transfer across domains?

## ğŸ”§ Implementation Roadmap

- [ ] **Phase 1**: Literature review and baseline implementation
- [ ] **Phase 2**: Core TIR-Judge framework development
- [ ] **Phase 3**: Experimental validation on benchmarks
- [ ] **Phase 4**: Ablation studies and analysis

## ğŸ“– Documentation

- **Research Guide**: See [CLAUDE.md](./CLAUDE.md) for detailed project context
- **Original Paper**: Available in `docs/original-docs/2510.23038v1.pdf`
- **Analysis Notes**: Research findings in `docs/analysis/`

## ğŸ¤ Contributing

This is a research project focused on understanding and extending TIR-Judge concepts:

1. Maintain research integrity and proper attribution
2. Follow systematic experimental practices
3. Document findings and methodologies
4. Use reproducible setups

## ğŸ“„ Citation

```bibtex
@article{xu2025tir,
  title={Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning},
  author={Xu, Ran and Chen, Jingjing and Ye, Jiayu and Wu, Yu and Yan, Jun and Yang, Carl and Yu, Hongkun},
  journal={arXiv preprint arXiv:2510.23038},
  year={2025}
}
```

## ğŸ”— Resources

- **Original Research**: Google Research & Emory University
- **Paper arXiv**: [2510.23038](https://arxiv.org/abs/2510.23038)
- **Related Work**: Tool-augmented LLMs, RL for NLP, LLM-as-a-judge

---

*A systematic exploration of tool-integrated reinforcement learning for more capable LLM judges.*